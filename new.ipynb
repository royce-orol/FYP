{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update this at the top of your modeling section\n",
    "results = []\n",
    "\n",
    "def store_result(branch, model_name, feature_strategy, tuning, accuracy, precision, recall, f1):\n",
    "    results.append({\n",
    "        'Branch': branch,\n",
    "        'Model': model_name,\n",
    "        'Feature Selection': feature_strategy,\n",
    "        'Tuned': tuning,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-score': f1\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4z/pg_w4ml11xvdtzv8kb6z7w7c0000gn/T/ipykernel_14285/1686993148.py:22: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "/var/folders/4z/pg_w4ml11xvdtzv8kb6z7w7c0000gn/T/ipykernel_14285/1686993148.py:19: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].replace(\"None\", np.nan, inplace=True)\n",
      "/var/folders/4z/pg_w4ml11xvdtzv8kb6z7w7c0000gn/T/ipykernel_14285/1686993148.py:20: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].mode()[0], inplace=True)\n",
      "/var/folders/4z/pg_w4ml11xvdtzv8kb6z7w7c0000gn/T/ipykernel_14285/1686993148.py:22: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "/var/folders/4z/pg_w4ml11xvdtzv8kb6z7w7c0000gn/T/ipykernel_14285/1686993148.py:19: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].replace(\"None\", np.nan, inplace=True)\n",
      "/var/folders/4z/pg_w4ml11xvdtzv8kb6z7w7c0000gn/T/ipykernel_14285/1686993148.py:20: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].mode()[0], inplace=True)\n",
      "/var/folders/4z/pg_w4ml11xvdtzv8kb6z7w7c0000gn/T/ipykernel_14285/1686993148.py:22: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "/var/folders/4z/pg_w4ml11xvdtzv8kb6z7w7c0000gn/T/ipykernel_14285/1686993148.py:19: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].replace(\"None\", np.nan, inplace=True)\n",
      "/var/folders/4z/pg_w4ml11xvdtzv8kb6z7w7c0000gn/T/ipykernel_14285/1686993148.py:20: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].mode()[0], inplace=True)\n",
      "/var/folders/4z/pg_w4ml11xvdtzv8kb6z7w7c0000gn/T/ipykernel_14285/1686993148.py:22: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "/var/folders/4z/pg_w4ml11xvdtzv8kb6z7w7c0000gn/T/ipykernel_14285/1686993148.py:19: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].replace(\"None\", np.nan, inplace=True)\n",
      "/var/folders/4z/pg_w4ml11xvdtzv8kb6z7w7c0000gn/T/ipykernel_14285/1686993148.py:20: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].mode()[0], inplace=True)\n",
      "/var/folders/4z/pg_w4ml11xvdtzv8kb6z7w7c0000gn/T/ipykernel_14285/1686993148.py:22: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "/var/folders/4z/pg_w4ml11xvdtzv8kb6z7w7c0000gn/T/ipykernel_14285/1686993148.py:19: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].replace(\"None\", np.nan, inplace=True)\n",
      "/var/folders/4z/pg_w4ml11xvdtzv8kb6z7w7c0000gn/T/ipykernel_14285/1686993148.py:20: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].mode()[0], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessing complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"nyc.csv\")  \n",
    "# Drop rows where AIDS_diagnosed is missing\n",
    "df = df.dropna(subset=[\"AIDS_diagnosed\"])\n",
    "\n",
    "# Replace 'None' with np.nan in Concurrent_diagnosed\n",
    "df['Concurrent_diagnosed'] = df['Concurrent_diagnosed'].replace('None', np.nan)\n",
    "df['Concurrent_diagnosed'] = df['Concurrent_diagnosed'].fillna('No Other Disease')\n",
    "\n",
    "# Fill missing or None values (mode for categorical, median for numeric)\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        df[col].replace(\"None\", np.nan, inplace=True)\n",
    "        df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "    else:\n",
    "        df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "# Convert boolean-like columns to 0/1\n",
    "bool_columns = ['HIV_diagnosed', 'AIDS_diagnosed', 'Linked_to_Care_3mo', 'Death_Status']\n",
    "for col in bool_columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        df[col] = df[col].map({'No': 0, 'Yes': 1, 'Alive': 0, 'Deceased': 1, True: 1, False: 0})\n",
    "    df[col] = df[col].astype(int)\n",
    "\n",
    "# Outlier detection (optional)\n",
    "from sklearn.ensemble import IsolationForest\n",
    "iso = IsolationForest(contamination=0.01, random_state=42)\n",
    "outliers = iso.fit_predict(df.select_dtypes(include=np.number))\n",
    "df = df[outliers == 1]\n",
    "\n",
    "# Split features and target\n",
    "X = df.drop('AIDS_diagnosed', axis=1)\n",
    "y = df['AIDS_diagnosed']\n",
    "\n",
    "# Encode categorical variables\n",
    "cat_columns = X.select_dtypes(include=['object']).columns\n",
    "le = LabelEncoder()\n",
    "for col in cat_columns:\n",
    "    X[col] = le.fit_transform(X[col])\n",
    "\n",
    "# === Handle class imbalance using SMOTE ===\n",
    "smote = SMOTE(random_state=42)\n",
    "X_bal, y_bal = smote.fit_resample(X, y)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Standard scaling for most models\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_bal)\n",
    "X = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_bal, test_size=0.2, random_state=42)\n",
    "\n",
    "# MinMax scaling for chi-square feature selection\n",
    "minmax_scaler = MinMaxScaler()\n",
    "X_bal_minmax = minmax_scaler.fit_transform(X_bal)\n",
    "X_minmax = pd.DataFrame(X_bal_minmax, columns=X.columns)\n",
    "X_train_minmax, X_test_minmax, y_train_minmax, y_test_minmax = train_test_split(X_minmax, y_bal, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_bal)\n",
    "X = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_bal, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"✅ Preprocessing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BRANCH A : TRAIN ON 80% TEST ON 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Branch A: 80% train, 20% test\n",
    "X_train_A, X_test_A, y_train_A, y_test_A = train_test_split(X_bal, y_bal, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standard scaling for most models\n",
    "scaler_A = StandardScaler()\n",
    "X_train_A_scaled = scaler_A.fit_transform(X_train_A)\n",
    "X_test_A_scaled = scaler_A.transform(X_test_A)\n",
    "X_train_A_scaled = pd.DataFrame(X_train_A_scaled, columns=X.columns)\n",
    "X_test_A_scaled = pd.DataFrame(X_test_A_scaled, columns=X.columns)\n",
    "\n",
    "# MinMax scaling for chi-square\n",
    "minmax_scaler_A = MinMaxScaler()\n",
    "X_train_A_minmax = minmax_scaler_A.fit_transform(X_train_A)\n",
    "X_test_A_minmax = minmax_scaler_A.transform(X_test_A)\n",
    "X_train_A_minmax = pd.DataFrame(X_train_A_minmax, columns=X.columns)\n",
    "X_test_A_minmax = pd.DataFrame(X_test_A_minmax, columns=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BRANCH B : TRAIN ON 20% TEST ON 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Branch B: 20% train, 80% test\n",
    "X_train_B, X_test_B, y_train_B, y_test_B = train_test_split(X_bal, y_bal, test_size=0.8, random_state=42)\n",
    "\n",
    "# Standard scaling for most models\n",
    "scaler_B = StandardScaler()\n",
    "X_train_B_scaled = scaler_B.fit_transform(X_train_B)\n",
    "X_test_B_scaled = scaler_B.transform(X_test_B)\n",
    "X_train_B_scaled = pd.DataFrame(X_train_B_scaled, columns=X.columns)\n",
    "X_test_B_scaled = pd.DataFrame(X_test_B_scaled, columns=X.columns)\n",
    "\n",
    "# MinMax scaling for chi-square\n",
    "minmax_scaler_B = MinMaxScaler()\n",
    "X_train_B_minmax = minmax_scaler_B.fit_transform(X_train_B)\n",
    "X_test_B_minmax = minmax_scaler_B.transform(X_test_B)\n",
    "X_train_B_minmax = pd.DataFrame(X_train_B_minmax, columns=X.columns)\n",
    "X_test_B_minmax = pd.DataFrame(X_test_B_minmax, columns=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEATURE SELECTION ARRAYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chi-square (use MinMax scaled data)\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "k = 10  # or any number you prefer\n",
    "\n",
    "# Branch A\n",
    "selector_A = SelectKBest(score_func=chi2, k=k)\n",
    "X_train_A_chi = selector_A.fit_transform(X_train_A_minmax, y_train_A)\n",
    "X_test_A_chi = selector_A.transform(X_test_A_minmax)\n",
    "\n",
    "# Branch B\n",
    "selector_B = SelectKBest(score_func=chi2, k=k)\n",
    "X_train_B_chi = selector_B.fit_transform(X_train_B_minmax, y_train_B)\n",
    "X_test_B_chi = selector_B.transform(X_test_B_minmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haziqzairul/.local/share/virtualenvs/datamining-5l9yRiuj/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:3045: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "/Users/haziqzairul/.local/share/virtualenvs/datamining-5l9yRiuj/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:3046: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n",
      "/Users/haziqzairul/.local/share/virtualenvs/datamining-5l9yRiuj/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:3045: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "/Users/haziqzairul/.local/share/virtualenvs/datamining-5l9yRiuj/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:3046: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    }
   ],
   "source": [
    "# Correlation (Use Standard scaled data)\n",
    "\n",
    "correlations_A = X_train_A_scaled.corrwith(pd.Series(y_train_A)).abs()\n",
    "top_k_A = correlations_A.sort_values(ascending=False).head(k).index.tolist()\n",
    "X_train_A_corr = X_train_A_scaled[top_k_A]\n",
    "X_test_A_corr = X_test_A_scaled[top_k_A]\n",
    "\n",
    "# Branch B\n",
    "correlations_B = X_train_B_scaled.corrwith(pd.Series(y_train_B)).abs()\n",
    "top_k_B = correlations_B.sort_values(ascending=False).head(k).index.tolist()\n",
    "X_train_B_corr = X_train_B_scaled[top_k_B]\n",
    "X_test_B_corr = X_test_B_scaled[top_k_B]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM BRANCH A (80/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Branch A (No Feature Selection, Not Tuned) Accuracy: 0.6902324461038178\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.76      0.71      4154\n",
      "           1       0.72      0.62      0.67      4149\n",
      "\n",
      "    accuracy                           0.69      8303\n",
      "   macro avg       0.69      0.69      0.69      8303\n",
      "weighted avg       0.69      0.69      0.69      8303\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SVM BRANCH A NO FEATURE SELECTION\n",
    "#NOT TUNED\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score\n",
    "\n",
    "svm_A = SVC(random_state=42)\n",
    "svm_A.fit(X_train_A_scaled, y_train_A)\n",
    "y_pred_svm_A = svm_A.predict(X_test_A_scaled)\n",
    "print(\"SVM Branch A (No Feature Selection, Not Tuned) Accuracy:\", accuracy_score(y_test_A, y_pred_svm_A))\n",
    "print(classification_report(y_test_A, y_pred_svm_A))\n",
    "store_result('A','SVM', 'None', 'No', accuracy_score(y_test_A, y_pred_svm_A),\n",
    "             precision_score(y_test_A, y_pred_svm_A, average='weighted', zero_division=0),\n",
    "             recall_score(y_test_A, y_pred_svm_A, average='weighted', zero_division=0),\n",
    "             f1_score(y_test_A, y_pred_svm_A, average='weighted', zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haziqzairul/.local/share/virtualenvs/datamining-5l9yRiuj/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:3045: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "/Users/haziqzairul/.local/share/virtualenvs/datamining-5l9yRiuj/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:3046: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# Chi-square feature selection\n",
    "\n",
    "k = 10  # or any number you prefer\n",
    "selector = SelectKBest(score_func=chi2, k=k)\n",
    "X_train_chi = selector.fit_transform(X_train_minmax, y_train_minmax)\n",
    "X_test_chi = selector.transform(X_test_minmax)\n",
    "\n",
    "\n",
    "# Correlation-based feature selection\n",
    "import numpy as np\n",
    "correlations = pd.DataFrame(X_train).corrwith(pd.Series(y_train)).abs()\n",
    "top_k = correlations.sort_values(ascending=False).head(k).index\n",
    "X_train_corr = X_train[top_k]\n",
    "X_test_corr = X_test[top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkernel\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrbf\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscale\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[1;32m      5\u001b[0m svm_A_tuned \u001b[38;5;241m=\u001b[39m GridSearchCV(SVC(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m), param_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43msvm_A_tuned\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_A_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_A\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m y_pred_svm_A_tuned \u001b[38;5;241m=\u001b[39m svm_A_tuned\u001b[38;5;241m.\u001b[39mpredict(X_test_A_scaled)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSVM Branch A (No Feature Selection, Tuned) Best Params:\u001b[39m\u001b[38;5;124m\"\u001b[39m, svm_A_tuned\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/datamining-5l9yRiuj/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/datamining-5l9yRiuj/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1019\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m   1014\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1015\u001b[0m     )\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m-> 1019\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/datamining-5l9yRiuj/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1573\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1572\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1573\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/datamining-5l9yRiuj/lib/python3.12/site-packages/sklearn/model_selection/_search.py:965\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    961\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    962\u001b[0m         )\n\u001b[1;32m    963\u001b[0m     )\n\u001b[0;32m--> 965\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    984\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    987\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    988\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/datamining-5l9yRiuj/lib/python3.12/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/datamining-5l9yRiuj/lib/python3.12/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/datamining-5l9yRiuj/lib/python3.12/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/datamining-5l9yRiuj/lib/python3.12/site-packages/sklearn/utils/parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/datamining-5l9yRiuj/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:888\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    886\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    887\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 888\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    891\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    892\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/datamining-5l9yRiuj/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/datamining-5l9yRiuj/lib/python3.12/site-packages/sklearn/svm/_base.py:250\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LibSVM]\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    249\u001b[0m seed \u001b[38;5;241m=\u001b[39m rnd\u001b[38;5;241m.\u001b[39mrandint(np\u001b[38;5;241m.\u001b[39miinfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmax)\n\u001b[0;32m--> 250\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# see comment on the other call to np.iinfo in this file\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape_fit_ \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m (n_samples,)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/datamining-5l9yRiuj/lib/python3.12/site-packages/sklearn/svm/_base.py:328\u001b[0m, in \u001b[0;36mBaseLibSVM._dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    314\u001b[0m libsvm\u001b[38;5;241m.\u001b[39mset_verbosity_wrap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# we don't pass **self.get_params() to allow subclasses to\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# add other parameters to __init__\u001b[39;00m\n\u001b[1;32m    318\u001b[0m (\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_,\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_vectors_,\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_support,\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdual_coef_,\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_,\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probA,\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probB,\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_status_,\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_iter,\n\u001b[0;32m--> 328\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mlibsvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43msvm_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclass_weight_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mC\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprobability\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdegree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdegree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshrinking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshrinking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoef0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoef0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_from_fit_status()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#TUNED\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf'], 'gamma': ['scale', 'auto']}\n",
    "svm_A_tuned = GridSearchCV(SVC(random_state=42), param_grid, cv=3, scoring='accuracy')\n",
    "svm_A_tuned.fit(X_train_A_scaled, y_train_A)\n",
    "y_pred_svm_A_tuned = svm_A_tuned.predict(X_test_A_scaled)\n",
    "print(\"SVM Branch A (No Feature Selection, Tuned) Best Params:\", svm_A_tuned.best_params_)\n",
    "print(\"Accuracy:\", accuracy_score(y_test_A, y_pred_svm_A_tuned))\n",
    "print(classification_report(y_test_A, y_pred_svm_A_tuned))\n",
    "store_result('A','SVM', 'None', 'Yes', accuracy_score(y_test_A, y_pred_svm_A_tuned),\n",
    "             precision_score(y_test_A, y_pred_svm_A_tuned, average='weighted', zero_division=0),\n",
    "             recall_score(y_test_A, y_pred_svm_A_tuned, average='weighted', zero_division=0),\n",
    "             f1_score(y_test_A, y_pred_svm_A_tuned, average='weighted', zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM BRANCH A CHI SQUARE FEATURE SELECTION\n",
    "# NOT TUNED\n",
    "\n",
    "svm_A_chi = SVC(random_state=42)\n",
    "svm_A_chi.fit(X_train_A_chi, y_train_A)\n",
    "y_pred_svm_A_chi = svm_A_chi.predict(X_test_A_chi)\n",
    "print(\"SVM Branch A (Chi-square, Not Tuned) Accuracy:\", accuracy_score(y_test_A, y_pred_svm_A_chi))\n",
    "print(classification_report(y_test_A, y_pred_svm_A_chi))\n",
    "store_result('A''SVM', 'Chi-square', 'No', accuracy_score(y_test_A, y_pred_svm_A_chi),\n",
    "             precision_score(y_test_A, y_pred_svm_A_chi, average='weighted', zero_division=0),\n",
    "             recall_score(y_test_A, y_pred_svm_A_chi, average='weighted', zero_division=0),\n",
    "             f1_score(y_test_A, y_pred_svm_A_chi, average='weighted', zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TUNED\n",
    "svm_A_chi_tuned = GridSearchCV(SVC(random_state=42), param_grid, cv=3, scoring='accuracy')\n",
    "svm_A_chi_tuned.fit(X_train_A_chi, y_train_A)\n",
    "y_pred_svm_A_chi_tuned = svm_A_chi_tuned.predict(X_test_A_chi)\n",
    "print(\"SVM Branch A (Chi-square, Tuned) Best Params:\", svm_A_chi_tuned.best_params_)\n",
    "print(\"Accuracy:\", accuracy_score(y_test_A, y_pred_svm_A_chi_tuned))\n",
    "print(classification_report(y_test_A, y_pred_svm_A_chi_tuned))\n",
    "store_result('A','SVM', 'Chi-square', 'Yes', accuracy_score(y_test_A, y_pred_svm_A_chi_tuned),\n",
    "             precision_score(y_test_A, y_pred_svm_A_chi_tuned, average='weighted', zero_division=0),\n",
    "             recall_score(y_test_A, y_pred_svm_A_chi_tuned, average='weighted', zero_division=0),\n",
    "             f1_score(y_test_A, y_pred_svm_A_chi_tuned, average='weighted', zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM BRANCH A CORRELATION FEATURE SELECTION\n",
    "# NOT TUNED\n",
    "svm_A_corr = SVC(random_state=42)\n",
    "svm_A_corr.fit(X_train_A_corr, y_train_A)\n",
    "y_pred_svm_A_corr = svm_A_corr.predict(X_test_A_corr)\n",
    "print(\"SVM Branch A (Correlation, Not Tuned) Accuracy:\", accuracy_score(y_test_A, y_pred_svm_A_corr))\n",
    "print(classification_report(y_test_A, y_pred_svm_A_corr))\n",
    "store_result('A','SVM', 'Correlation', 'No', accuracy_score(y_test_A, y_pred_svm_A_corr),\n",
    "             precision_score(y_test_A, y_pred_svm_A_corr, average='weighted', zero_division=0),\n",
    "             recall_score(y_test_A, y_pred_svm_A_corr, average='weighted', zero_division=0),\n",
    "             f1_score(y_test_A, y_pred_svm_A_corr, average='weighted', zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TUNED\n",
    "svm_A_corr_tuned = GridSearchCV(SVC(random_state=42), param_grid, cv=3, scoring='accuracy')\n",
    "svm_A_corr_tuned.fit(X_train_A_corr, y_train_A)\n",
    "y_pred_svm_A_corr_tuned = svm_A_corr_tuned.predict(X_test_A_corr)\n",
    "print(\"SVM Branch A (Correlation, Tuned) Best Params:\", svm_A_corr_tuned.best_params_)\n",
    "print(\"Accuracy:\", accuracy_score(y_test_A, y_pred_svm_A_corr_tuned))\n",
    "print(classification_report(y_test_A, y_pred_svm_A_corr_tuned))\n",
    "store_result('A','SVM', 'Correlation', 'Yes', accuracy_score(y_test_A, y_pred_svm_A_corr_tuned),\n",
    "             precision_score(y_test_A, y_pred_svm_A_corr_tuned, average='weighted', zero_division=0),\n",
    "             recall_score(y_test_A, y_pred_svm_A_corr_tuned, average='weighted', zero_division=0),\n",
    "             f1_score(y_test_A, y_pred_svm_A_corr_tuned, average='weighted', zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM BRANCH B (20/80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NO FEATURE SELECTION\n",
    "#NOT TUNED\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score\n",
    "\n",
    "svm_B = SVC(random_state=42)\n",
    "svm_B.fit(X_train_B_scaled, y_train_B)\n",
    "y_pred_svm_B = svm_B.predict(X_test_B_scaled)\n",
    "print(\"SVM Branch B (No Feature Selection, Not Tuned) Accuracy:\", accuracy_score(y_test_B, y_pred_svm_B))\n",
    "print(classification_report(y_test_B, y_pred_svm_B))\n",
    "store_result('B','SVM', 'None', 'No', accuracy_score(y_test_B, y_pred_svm_B),\n",
    "             precision_score(y_test_B, y_pred_svm_B, average='weighted', zero_division=0),\n",
    "             recall_score(y_test_B, y_pred_svm_B, average='weighted', zero_division=0),\n",
    "             f1_score(y_test_B, y_pred_svm_B, average='weighted', zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TUNED\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf'], 'gamma': ['scale', 'auto']}\n",
    "svm_B_tuned = GridSearchCV(SVC(random_state=42), param_grid, cv=3, scoring='accuracy')\n",
    "svm_B_tuned.fit(X_train_B_scaled, y_train_B)\n",
    "y_pred_svm_B_tuned = svm_B_tuned.predict(X_test_B_scaled)\n",
    "print(\"SVM Branch B (No Feature Selection, Tuned) Best Params:\", svm_B_tuned.best_params_)\n",
    "print(\"Accuracy:\", accuracy_score(y_test_B, y_pred_svm_B_tuned))\n",
    "print(classification_report(y_test_B, y_pred_svm_B_tuned))\n",
    "store_result('B','SVM', 'None', 'Yes', accuracy_score(y_test_B, y_pred_svm_B_tuned),\n",
    "             precision_score(y_test_B, y_pred_svm_B_tuned, average='weighted', zero_division=0),\n",
    "             recall_score(y_test_B, y_pred_svm_B_tuned, average='weighted', zero_division=0),\n",
    "             f1_score(y_test_B, y_pred_svm_B_tuned, average='weighted', zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHI SQUARE FEATURE SELECTION\n",
    "#NOT TUNED\n",
    "\n",
    "svm_B_chi = SVC(random_state=42)\n",
    "svm_B_chi.fit(X_train_B_chi, y_train_B)\n",
    "y_pred_svm_B_chi = svm_B_chi.predict(X_test_B_chi)\n",
    "print(\"SVM Branch B (Chi-square, Not Tuned) Accuracy:\", accuracy_score(y_test_B, y_pred_svm_B_chi))\n",
    "print(classification_report(y_test_B, y_pred_svm_B_chi))\n",
    "store_result('B','SVM', 'Chi-square', 'No', accuracy_score(y_test_B, y_pred_svm_B_chi),\n",
    "             precision_score(y_test_B, y_pred_svm_B_chi, average='weighted', zero_division=0),\n",
    "             recall_score(y_test_B, y_pred_svm_B_chi, average='weighted', zero_division=0),\n",
    "             f1_score(y_test_B, y_pred_svm_B_chi, average='weighted', zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TUNED\n",
    "svm_B_chi_tuned = GridSearchCV(SVC(random_state=42), param_grid, cv=3, scoring='accuracy')\n",
    "svm_B_chi_tuned.fit(X_train_B_chi, y_train_B)\n",
    "y_pred_svm_B_chi_tuned = svm_B_chi_tuned.predict(X_test_B_chi)\n",
    "print(\"SVM Branch B (Chi-square, Tuned) Best Params:\", svm_B_chi_tuned.best_params_)\n",
    "print(\"Accuracy:\", accuracy_score(y_test_B, y_pred_svm_B_chi_tuned))\n",
    "print(classification_report(y_test_B, y_pred_svm_B_chi_tuned))\n",
    "store_result('B','SVM', 'Chi-square', 'Yes', accuracy_score(y_test_B, y_pred_svm_B_chi_tuned),\n",
    "             precision_score(y_test_B, y_pred_svm_B_chi_tuned, average='weighted', zero_division=0),\n",
    "             recall_score(y_test_B, y_pred_svm_B_chi_tuned, average='weighted', zero_division=0),\n",
    "             f1_score(y_test_B, y_pred_svm_B_chi_tuned, average='weighted', zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CORRELATION FEATURE SELECTION\n",
    "#NOT TUNED\n",
    "svm_B_corr = SVC(random_state=42)\n",
    "svm_B_corr.fit(X_train_B_corr, y_train_B)\n",
    "y_pred_svm_B_corr = svm_B_corr.predict(X_test_B_corr)\n",
    "print(\"SVM Branch B (Correlation, Not Tuned) Accuracy:\", accuracy_score(y_test_B, y_pred_svm_B_corr))\n",
    "print(classification_report(y_test_B, y_pred_svm_B_corr))\n",
    "store_result('B','SVM', 'Correlation', 'No', accuracy_score(y_test_B, y_pred_svm_B_corr),\n",
    "             precision_score(y_test_B, y_pred_svm_B_corr, average='weighted', zero_division=0),\n",
    "             recall_score(y_test_B, y_pred_svm_B_corr, average='weighted', zero_division=0),\n",
    "             f1_score(y_test_B, y_pred_svm_B_corr, average='weighted', zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TUNED\n",
    "svm_B_corr_tuned = GridSearchCV(SVC(random_state=42), param_grid, cv=3, scoring='accuracy')\n",
    "svm_B_corr_tuned.fit(X_train_B_corr, y_train_B)\n",
    "y_pred_svm_B_corr_tuned = svm_B_corr_tuned.predict(X_test_B_corr)\n",
    "print(\"SVM Branch B (Correlation, Tuned) Best Params:\", svm_B_corr_tuned.best_params_)\n",
    "print(\"Accuracy:\", accuracy_score(y_test_B, y_pred_svm_B_corr_tuned))\n",
    "print(classification_report(y_test_B, y_pred_svm_B_corr_tuned))\n",
    "store_result('B','SVM', 'Correlation', 'Yes', accuracy_score(y_test_B, y_pred_svm_B_corr_tuned),\n",
    "             precision_score(y_test_B, y_pred_svm_B_corr_tuned, average='weighted', zero_division=0),\n",
    "             recall_score(y_test_B, y_pred_svm_B_corr_tuned, average='weighted', zero_division=0),\n",
    "             f1_score(y_test_B, y_pred_svm_B_corr_tuned, average='weighted', zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DECISION TREE BRANCH A (80/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NO FEATURE SELECTION\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score\n",
    "\n",
    "# NOT TUNED\n",
    "dt_A = DecisionTreeClassifier(random_state=42)\n",
    "dt_A.fit(X_train_A_scaled, y_train_A)\n",
    "y_pred_dt_A = dt_A.predict(X_test_A_scaled)\n",
    "print(\"Decision Tree Branch A (No Feature Selection, Not Tuned) Accuracy:\", accuracy_score(y_test_A, y_pred_dt_A))\n",
    "print(classification_report(y_test_A, y_pred_dt_A))\n",
    "store_result('Decision Tree', 'None', 'No', accuracy_score(y_test_A, y_pred_dt_A),\n",
    "             precision_score(y_test_A, y_pred_dt_A, average='weighted', zero_division=0),\n",
    "             recall_score(y_test_A, y_pred_dt_A, average='weighted', zero_division=0),\n",
    "             f1_score(y_test_A, y_pred_dt_A, average='weighted', zero_division=0))\n",
    "\n",
    "# Tuned\n",
    "param_grid_dt = {'max_depth': [3, 5, 10, None], 'min_samples_split': [2, 5, 10]}\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "dt_A_tuned = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid_dt, cv=3, scoring='accuracy')\n",
    "dt_A_tuned.fit(X_train_A_scaled, y_train_A)\n",
    "y_pred_dt_A_tuned = dt_A_tuned.predict(X_test_A_scaled)\n",
    "print(\"Decision Tree Branch A (No Feature Selection, Tuned) Best Params:\", dt_A_tuned.best_params_)\n",
    "print(\"Accuracy:\", accuracy_score(y_test_A, y_pred_dt_A_tuned))\n",
    "print(classification_report(y_test_A, y_pred_dt_A_tuned))\n",
    "store_result('A','Decision Tree', 'None', 'Yes', accuracy_score(y_test_A, y_pred_dt_A_tuned),\n",
    "             precision_score(y_test_A, y_pred_dt_A_tuned, average='weighted', zero_division=0),\n",
    "             recall_score(y_test_A, y_pred_dt_A_tuned, average='weighted', zero_division=0),\n",
    "             f1_score(y_test_A, y_pred_dt_A_tuned, average='weighted', zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DecisionTreeClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# CHI SQUARE FEATURE SELECTION\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m dt_A_chi \u001b[38;5;241m=\u001b[39m \u001b[43mDecisionTreeClassifier\u001b[49m(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      4\u001b[0m dt_A_chi\u001b[38;5;241m.\u001b[39mfit(X_train_A_chi, y_train_A)\n\u001b[1;32m      5\u001b[0m y_pred_dt_A_chi \u001b[38;5;241m=\u001b[39m dt_A_chi\u001b[38;5;241m.\u001b[39mpredict(X_test_A_chi)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DecisionTreeClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "# CHI SQUARE FEATURE SELECTION\n",
    "\n",
    "#NOT TUNED\n",
    "dt_A_chi = DecisionTreeClassifier(random_state=42)\n",
    "dt_A_chi.fit(X_train_A_chi, y_train_A)\n",
    "y_pred_dt_A_chi = dt_A_chi.predict(X_test_A_chi)\n",
    "print(\"Decision Tree Branch A (Chi-square, Not Tuned) Accuracy:\", accuracy_score(y_test_A, y_pred_dt_A_chi))\n",
    "print(classification_report(y_test_A, y_pred_dt_A_chi))\n",
    "store_result('Decision Tree', 'Chi-square', 'No', accuracy_score(y_test_A, y_pred_dt_A_chi),\n",
    "             precision_score(y_test_A, y_pred_dt_A_chi, average='weighted', zero_division=0),\n",
    "             recall_score(y_test_A, y_pred_dt_A_chi, average='weighted', zero_division=0),\n",
    "             f1_score(y_test_A, y_pred_dt_A_chi, average='weighted', zero_division=0))\n",
    "\n",
    "# Tuned\n",
    "dt_A_chi_tuned = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid_dt, cv=3, scoring='accuracy')\n",
    "dt_A_chi_tuned.fit(X_train_A_chi, y_train_A)\n",
    "y_pred_dt_A_chi_tuned = dt_A_chi_tuned.predict(X_test_A_chi)\n",
    "print(\"Decision Tree Branch A (Chi-square, Tuned) Best Params:\", dt_A_chi_tuned.best_params_)\n",
    "print(\"Accuracy:\", accuracy_score(y_test_A, y_pred_dt_A_chi_tuned))\n",
    "print(classification_report(y_test_A, y_pred_dt_A_chi_tuned))\n",
    "store_result('A','Decision Tree', 'Chi-square', 'Yes', accuracy_score(y_test_A, y_pred_dt_A_chi_tuned),\n",
    "             precision_score(y_test_A, y_pred_dt_A_chi_tuned, average='weighted', zero_division=0),\n",
    "             recall_score(y_test_A, y_pred_dt_A_chi_tuned, average='weighted', zero_division=0),\n",
    "             f1_score(y_test_A, y_pred_dt_A_chi_tuned, average='weighted', zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CORRELATION FEATURE SELECTION\n",
    "# Not Tuned\n",
    "dt_A_corr = DecisionTreeClassifier(random_state=42)\n",
    "dt_A_corr.fit(X_train_A_corr, y_train_A)\n",
    "y_pred_dt_A_corr = dt_A_corr.predict(X_test_A_corr)\n",
    "print(\"Decision Tree Branch A (Correlation, Not Tuned) Accuracy:\", accuracy_score(y_test_A, y_pred_dt_A_corr))\n",
    "print(classification_report(y_test_A, y_pred_dt_A_corr))\n",
    "store_result('A','Decision Tree', 'Correlation', 'No', accuracy_score(y_test_A, y_pred_dt_A_corr),\n",
    "             precision_score(y_test_A, y_pred_dt_A_corr, average='weighted', zero_division=0),\n",
    "             recall_score(y_test_A, y_pred_dt_A_corr, average='weighted', zero_division=0),\n",
    "             f1_score(y_test_A, y_pred_dt_A_corr, average='weighted', zero_division=0))\n",
    "\n",
    "# Tuned\n",
    "dt_A_corr_tuned = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid_dt, cv=3, scoring='accuracy')\n",
    "dt_A_corr_tuned.fit(X_train_A_corr, y_train_A)\n",
    "y_pred_dt_A_corr_tuned = dt_A_corr_tuned.predict(X_test_A_corr)\n",
    "print(\"Decision Tree Branch A (Correlation, Tuned) Best Params:\", dt_A_corr_tuned.best_params_)\n",
    "print(\"Accuracy:\", accuracy_score(y_test_A, y_pred_dt_A_corr_tuned))\n",
    "print(classification_report(y_test_A, y_pred_dt_A_corr_tuned))\n",
    "store_result('Decision Tree', 'Correlation', 'Yes', accuracy_score(y_test_A, y_pred_dt_A_corr_tuned),\n",
    "             precision_score(y_test_A, y_pred_dt_A_corr_tuned, average='weighted', zero_division=0),\n",
    "             recall_score(y_test_A, y_pred_dt_A_corr_tuned, average='weighted', zero_division=0),\n",
    "             f1_score(y_test_A, y_pred_dt_A_corr_tuned, average='weighted', zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DECISION TREE BRANCH B (20/80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NO FEATURE SELECTION\n",
    "#NOT TUNED\n",
    "\n",
    "dt_B = DecisionTreeClassifier(random_state=42)\n",
    "dt_B.fit(X_train_B_scaled, y_train_B)\n",
    "y_pred_dt_B = dt_B.predict(X_test_B_scaled)\n",
    "print(\"Decision Tree Branch B (No Feature Selection, Not Tuned) Accuracy:\", accuracy_score(y_test_B, y_pred_dt_B))\n",
    "print(classification_report(y_test_B, y_pred_dt_B))\n",
    "store_result('B','Decision Tree', 'None', 'No', accuracy_score(y_test_B, y_pred_dt_B),\n",
    "             precision_score(y_test_B, y_pred_dt_B, average='weighted', zero_division=0),\n",
    "             recall_score(y_test_B, y_pred_dt_B, average='weighted', zero_division=0),\n",
    "             f1_score(y_test_B, y_pred_dt_B, average='weighted', zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TUNED\n",
    "dt_B_tuned = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid_dt, cv=3, scoring='accuracy')\n",
    "dt_B_tuned.fit(X_train_B_scaled, y_train_B)\n",
    "y_pred_dt_B_tuned = dt_B_tuned.predict(X_test_B_scaled)\n",
    "print(\"Decision Tree Branch B (No Feature Selection, Tuned) Best Params:\", dt_B_tuned.best_params_)\n",
    "print(\"Accuracy:\", accuracy_score(y_test_B, y_pred_dt_B_tuned))\n",
    "print(classification_report(y_test_B, y_pred_dt_B_tuned))\n",
    "store_result('B','Decision Tree', 'None', 'Yes', accuracy_score(y_test_B, y_pred_dt_B_tuned),\n",
    "             precision_score(y_test_B, y_pred_dt_B_tuned, average='weighted', zero_division=0),\n",
    "             recall_score(y_test_B, y_pred_dt_B_tuned, average='weighted', zero_division=0),\n",
    "             f1_score(y_test_B, y_pred_dt_B_tuned, average='weighted', zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHI SQUARE FEATURE SELECTION\n",
    "#NOT TUNED\n",
    "dt_B_chi = DecisionTreeClassifier(random_state=42)\n",
    "dt_B_chi.fit(X_train_B_chi, y_train_B)\n",
    "y_pred_dt_B_chi = dt_B_chi.predict(X_test_B_chi)\n",
    "print(\"Decision Tree Branch B (Chi-square, Not Tuned) Accuracy:\", accuracy_score(y_test_B, y_pred_dt_B_chi))\n",
    "print(classification_report(y_test_B, y_pred_dt_B_chi))\n",
    "store_result('B','Decision Tree', 'Chi-square', 'No', accuracy_score(y_test_B, y_pred_dt_B_chi),\n",
    "             precision_score(y_test_B, y_pred_dt_B_chi, average='weighted', zero_division=0),\n",
    "             recall_score(y_test_B, y_pred_dt_B_chi, average='weighted', zero_division=0),\n",
    "             f1_score(y_test_B, y_pred_dt_B_chi, average='weighted', zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TUNED\n",
    "dt_B_chi_tuned = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid_dt, cv=3, scoring='accuracy')\n",
    "dt_B_chi_tuned.fit(X_train_B_chi, y_train_B)\n",
    "y_pred_dt_B_chi_tuned = dt_B_chi_tuned.predict(X_test_B_chi)\n",
    "print(\"Decision Tree Branch B (Chi-square, Tuned) Best Params:\", dt_B_chi_tuned.best_params_)\n",
    "print(\"Accuracy:\", accuracy_score(y_test_B, y_pred_dt_B_chi_tuned))\n",
    "print(classification_report(y_test_B, y_pred_dt_B_chi_tuned))\n",
    "store_result('B','Decision Tree', 'Chi-square', 'Yes', accuracy_score(y_test_B, y_pred_dt_B_chi_tuned),\n",
    "             precision_score(y_test_B, y_pred_dt_B_chi_tuned, average='weighted', zero_division=0),\n",
    "             recall_score(y_test_B, y_pred_dt_B_chi_tuned, average='weighted', zero_division=0),\n",
    "             f1_score(y_test_B, y_pred_dt_B_chi_tuned, average='weighted', zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CORRELATION FEATURE SELECTION\n",
    "#NOT TUNED\n",
    "dt_B_corr = DecisionTreeClassifier(random_state=42)\n",
    "dt_B_corr.fit(X_train_B_corr, y_train_B)\n",
    "y_pred_dt_B_corr = dt_B_corr.predict(X_test_B_corr)\n",
    "print(\"Decision Tree Branch B (Correlation, Not Tuned) Accuracy:\", accuracy_score(y_test_B, y_pred_dt_B_corr))\n",
    "print(classification_report(y_test_B, y_pred_dt_B_corr))\n",
    "store_result('B','Decision Tree', 'Correlation', 'No', accuracy_score(y_test_B, y_pred_dt_B_corr),\n",
    "             precision_score(y_test_B, y_pred_dt_B_corr, average='weighted', zero_division=0),\n",
    "             recall_score(y_test_B, y_pred_dt_B_corr, average='weighted', zero_division=0),\n",
    "             f1_score(y_test_B, y_pred_dt_B_corr, average='weighted', zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TUNED\n",
    "dt_B_corr_tuned = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid_dt, cv=3, scoring='accuracy')\n",
    "dt_B_corr_tuned.fit(X_train_B_corr, y_train_B)\n",
    "y_pred_dt_B_corr_tuned = dt_B_corr_tuned.predict(X_test_B_corr)\n",
    "print(\"Decision Tree Branch B (Correlation, Tuned) Best Params:\", dt_B_corr_tuned.best_params_)\n",
    "print(\"Accuracy:\", accuracy_score(y_test_B, y_pred_dt_B_corr_tuned))\n",
    "print(classification_report(y_test_B, y_pred_dt_B_corr_tuned))\n",
    "store_result('B','Decision Tree', 'Correlation', 'Yes', accuracy_score(y_test_B, y_pred_dt_B_corr_tuned),\n",
    "             precision_score(y_test_B, y_pred_dt_B_corr_tuned, average='weighted', zero_division=0),\n",
    "             recall_score(y_test_B, y_pred_dt_B_corr_tuned, average='weighted', zero_division=0),\n",
    "             f1_score(y_test_B, y_pred_dt_B_corr_tuned, average='weighted', zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST BRANCH A (80/20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NO FEATURE SELECTION\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Not Tuned\n",
    "xgb_A = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_A.fit(X_train_A_scaled, y_train_A)\n",
    "y_pred_xgb_A = xgb_A.predict(X_test_A_scaled)\n",
    "print(\"XGBoost Branch A (No Feature Selection, Not Tuned) Accuracy:\", accuracy_score(y_test_A, y_pred_xgb_A))\n",
    "print(classification_report(y_test_A, y_pred_xgb_A))\n",
    "store_result('A','XGBoost', 'None', 'No', accuracy_score(y_test_A, y_pred_xgb_A),\n",
    "             precision_score(y_test_A, y_pred_xgb_A, average='weighted', zero_division=0),\n",
    "             recall_score(y_test_A, y_pred_xgb_A, average='weighted', zero_division=0),\n",
    "             f1_score(y_test_A, y_pred_xgb_A, average='weighted', zero_division=0))\n",
    "\n",
    "# Tuned\n",
    "param_grid_xgb = {'max_depth': [3, 5, 10], 'learning_rate': [0.01, 0.1, 0.2], 'n_estimators': [100, 200]}\n",
    "xgb_A_tuned = GridSearchCV(XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'), param_grid_xgb, cv=3, scoring='accuracy')\n",
    "xgb_A_tuned.fit(X_train_A_scaled, y_train_A)\n",
    "y_pred_xgb_A_tuned = xgb_A_tuned.predict(X_test_A_scaled)\n",
    "print(\"XGBoost Branch A (No Feature Selection, Tuned) Best Params:\", xgb_A_tuned.best_params_)\n",
    "print(\"Accuracy:\", accuracy_score(y_test_A, y_pred_xgb_A_tuned))\n",
    "print(classification_report(y_test_A, y_pred_xgb_A_tuned))\n",
    "store_result('A','XGBoost', 'None', 'Yes', accuracy_score(y_test_A, y_pred_xgb_A_tuned),\n",
    "             precision_score(y_test_A, y_pred_xgb_A_tuned, average='weighted', zero_division=0),\n",
    "             recall_score(y_test_A, y_pred_xgb_A_tuned, average='weighted', zero_division=0),\n",
    "             f1_score(y_test_A, y_pred_xgb_A_tuned, average='weighted', zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHI SQUARE FEATURE SELECTION\n",
    "\n",
    "# Not Tuned\n",
    "xgb_A_chi = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_A_chi.fit(X_train_A_chi, y_train_A)\n",
    "y_pred_xgb_A_chi = xgb_A_chi.predict(X_test_A_chi)\n",
    "print(\"XGBoost Branch A (Chi-square, Not Tuned) Accuracy:\", accuracy_score(y_test_A, y_pred_xgb_A_chi))\n",
    "print(classification_report(y_test_A, y_pred_xgb_A_chi))\n",
    "store_result('XGBoost', 'Chi-square', 'No', accuracy_score(y_test_A, y_pred_xgb_A_chi),\n",
    "             precision_score(y_test_A, y_pred_xgb_A_chi, average='weighted', zero_division=0),\n",
    "             recall_score(y_test_A, y_pred_xgb_A_chi, average='weighted', zero_division=0),\n",
    "             f1_score(y_test_A, y_pred_xgb_A_chi, average='weighted', zero_division=0))\n",
    "\n",
    "# Tuned\n",
    "xgb_A_chi_tuned = GridSearchCV(XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'), param_grid_xgb, cv=3, scoring='accuracy')\n",
    "xgb_A_chi_tuned.fit(X_train_A_chi, y_train_A)\n",
    "y_pred_xgb_A_chi_tuned = xgb_A_chi_tuned.predict(X_test_A_chi)\n",
    "print(\"XGBoost Branch A (Chi-square, Tuned) Best Params:\", xgb_A_chi_tuned.best_params_)\n",
    "print(\"Accuracy:\", accuracy_score(y_test_A, y_pred_xgb_A_chi_tuned))\n",
    "print(classification_report(y_test_A, y_pred_xgb_A_chi_tuned))\n",
    "store_result('XGBoost', 'Chi-square', 'Yes', accuracy_score(y_test_A, y_pred_xgb_A_chi_tuned),\n",
    "             precision_score(y_test_A, y_pred_xgb_A_chi_tuned, average='weighted', zero_division=0),\n",
    "             recall_score(y_test_A, y_pred_xgb_A_chi_tuned, average='weighted', zero_division=0),\n",
    "             f1_score(y_test_A, y_pred_xgb_A_chi_tuned, average='weighted', zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CORRELATION FEATURE SELECTION\n",
    "# Not Tuned\n",
    "xgb_A_corr = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_A_corr.fit(X_train_A_corr, y_train_A)\n",
    "y_pred_xgb_A_corr = xgb_A_corr.predict(X_test_A_corr)\n",
    "print(\"XGBoost Branch A (Correlation, Not Tuned) Accuracy:\", accuracy_score(y_test_A, y_pred_xgb_A_corr))\n",
    "print(classification_report(y_test_A, y_pred_xgb_A_corr))\n",
    "store_result('XGBoost', 'Correlation', 'No', accuracy_score(y_test_A, y_pred_xgb_A_corr),\n",
    "             precision_score(y_test_A, y_pred_xgb_A_corr, average='weighted', zero_division=0),\n",
    "             recall_score(y_test_A, y_pred_xgb_A_corr, average='weighted', zero_division=0),\n",
    "             f1_score(y_test_A, y_pred_xgb_A_corr, average='weighted', zero_division=0))\n",
    "\n",
    "# Tuned\n",
    "xgb_A_corr_tuned = GridSearchCV(XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'), param_grid_xgb, cv=3, scoring='accuracy')\n",
    "xgb_A_corr_tuned.fit(X_train_A_corr, y_train_A)\n",
    "y_pred_xgb_A_corr_tuned = xgb_A_corr_tuned.predict(X_test_A_corr)\n",
    "print(\"XGBoost Branch A (Correlation, Tuned) Best Params:\", xgb_A_corr_tuned.best_params_)\n",
    "print(\"Accuracy:\", accuracy_score(y_test_A, y_pred_xgb_A_corr_tuned))\n",
    "print(classification_report(y_test_A, y_pred_xgb_A_corr_tuned))\n",
    "store_result('XGBoost', 'Correlation', 'Yes', accuracy_score(y_test_A, y_pred_xgb_A_corr_tuned),\n",
    "             precision_score(y_test_A, y_pred_xgb_A_corr_tuned, average='weighted', zero_division=0),\n",
    "             recall_score(y_test_A, y_pred_xgb_A_corr_tuned, average='weighted', zero_division=0),\n",
    "             f1_score(y_test_A, y_pred_xgb_A_corr_tuned, average='weighted', zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST BRANCH B (20/80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FEATURE SELECTION\n",
    "#NOT TUNED \n",
    "xgb_B = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_B.fit(X_train_B_scaled, y_train_B)\n",
    "y_pred_xgb_B = xgb_B.predict(X_test_B_scaled)\n",
    "print(\"XGBoost Branch B (No Feature Selection, Not Tuned) Accuracy:\", accuracy_score(y_test_B, y_pred_xgb_B))\n",
    "print(classification_report(y_test_B, y_pred_xgb_B))\n",
    "store_result('B','XGBoost', 'None', 'No', accuracy_score(y_test_B, y_pred_xgb_B),\n",
    "             precision_score(y_test_B, y_pred_xgb_B, average='weighted', zero_division=0),\n",
    "             recall_score(y_test_B, y_pred_xgb_B, average='weighted', zero_division=0),\n",
    "             f1_score(y_test_B, y_pred_xgb_B, average='weighted', zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TUNED\n",
    "xgb_B_tuned = GridSearchCV(XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'), param_grid_xgb, cv=3, scoring='accuracy')\n",
    "xgb_B_tuned.fit(X_train_B_scaled, y_train_B)\n",
    "y_pred_xgb_B_tuned = xgb_B_tuned.predict(X_test_B_scaled)\n",
    "print(\"XGBoost Branch B (No Feature Selection, Tuned) Best Params:\", xgb_B_tuned.best_params_)\n",
    "print(\"Accuracy:\", accuracy_score(y_test_B, y_pred_xgb_B_tuned))\n",
    "print(classification_report(y_test_B, y_pred_xgb_B_tuned))\n",
    "store_result('B','XGBoost', 'None', 'Yes', accuracy_score(y_test_B, y_pred_xgb_B_tuned),\n",
    "             precision_score(y_test_B, y_pred_xgb_B_tuned, average='weighted', zero_division=0),\n",
    "             recall_score(y_test_B, y_pred_xgb_B_tuned, average='weighted', zero_division=0),\n",
    "             f1_score(y_test_B, y_pred_xgb_B_tuned, average='weighted', zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHI SQUARE FEATURE SELECTION\n",
    "# NOT TUNED\n",
    "\n",
    "xgb_B_chi = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_B_chi.fit(X_train_B_chi, y_train_B)\n",
    "y_pred_xgb_B_chi = xgb_B_chi.predict(X_test_B_chi)\n",
    "print(\"XGBoost Branch B (Chi-square, Not Tuned) Accuracy:\", accuracy_score(y_test_B, y_pred_xgb_B_chi))\n",
    "print(classification_report(y_test_B, y_pred_xgb_B_chi))\n",
    "store_result('B','XGBoost', 'Chi-square', 'No', accuracy_score(y_test_B, y_pred_xgb_B_chi),\n",
    "             precision_score(y_test_B, y_pred_xgb_B_chi, average='weighted', zero_division=0),\n",
    "             recall_score(y_test_B, y_pred_xgb_B_chi, average='weighted', zero_division=0),\n",
    "             f1_score(y_test_B, y_pred_xgb_B_chi, average='weighted', zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TUNED\n",
    "xgb_B_chi_tuned = GridSearchCV(XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'), param_grid_xgb, cv=3, scoring='accuracy')\n",
    "xgb_B_chi_tuned.fit(X_train_B_chi, y_train_B)\n",
    "y_pred_xgb_B_chi_tuned = xgb_B_chi_tuned.predict(X_test_B_chi)\n",
    "print(\"XGBoost Branch B (Chi-square, Tuned) Best Params:\", xgb_B_chi_tuned.best_params_)\n",
    "print(\"Accuracy:\", accuracy_score(y_test_B, y_pred_xgb_B_chi_tuned))\n",
    "print(classification_report(y_test_B, y_pred_xgb_B_chi_tuned))\n",
    "store_result('B','XGBoost', 'Chi-square', 'Yes', accuracy_score(y_test_B, y_pred_xgb_B_chi_tuned),\n",
    "             precision_score(y_test_B, y_pred_xgb_B_chi_tuned, average='weighted', zero_division=0),\n",
    "             recall_score(y_test_B, y_pred_xgb_B_chi_tuned, average='weighted', zero_division=0),\n",
    "             f1_score(y_test_B, y_pred_xgb_B_chi_tuned, average='weighted', zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRELATION FEATURE SELECTION\n",
    "# NOT TUNED\n",
    "xgb_B_corr = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_B_corr.fit(X_train_B_corr, y_train_B)\n",
    "y_pred_xgb_B_corr = xgb_B_corr.predict(X_test_B_corr)\n",
    "print(\"XGBoost Branch B (Correlation, Not Tuned) Accuracy:\", accuracy_score(y_test_B, y_pred_xgb_B_corr))\n",
    "print(classification_report(y_test_B, y_pred_xgb_B_corr))\n",
    "store_result('B','XGBoost', 'Correlation', 'No', accuracy_score(y_test_B, y_pred_xgb_B_corr),\n",
    "             precision_score(y_test_B, y_pred_xgb_B_corr, average='weighted', zero_division=0),\n",
    "             recall_score(y_test_B, y_pred_xgb_B_corr, average='weighted', zero_division=0),\n",
    "             f1_score(y_test_B, y_pred_xgb_B_corr, average='weighted', zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TUNED\n",
    "xgb_B_corr_tuned = GridSearchCV(XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'), param_grid_xgb, cv=3, scoring='accuracy')\n",
    "xgb_B_corr_tuned.fit(X_train_B_corr, y_train_B)\n",
    "y_pred_xgb_B_corr_tuned = xgb_B_corr_tuned.predict(X_test_B_corr)\n",
    "print(\"XGBoost Branch B (Correlation, Tuned) Best Params:\", xgb_B_corr_tuned.best_params_)\n",
    "print(\"Accuracy:\", accuracy_score(y_test_B, y_pred_xgb_B_corr_tuned))\n",
    "print(classification_report(y_test_B, y_pred_xgb_B_corr_tuned))\n",
    "store_result('B','XGBoost', 'Correlation', 'Yes', accuracy_score(y_test_B, y_pred_xgb_B_corr_tuned),\n",
    "             precision_score(y_test_B, y_pred_xgb_B_corr_tuned, average='weighted', zero_division=0),\n",
    "             recall_score(y_test_B, y_pred_xgb_B_corr_tuned, average='weighted', zero_division=0),\n",
    "             f1_score(y_test_B, y_pred_xgb_B_corr_tuned, average='weighted', zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATION COMPARISON TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create DataFrame from results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Sort for readability\n",
    "results_df = results_df.sort_values(by=['Branch', 'Model', 'Feature Selection', 'Tuned'])\n",
    "\n",
    "# Display the table\n",
    "display(results_df)\n",
    "\n",
    "# Optionally, highlight the best accuracy and F1-score in each branch/model\n",
    "def highlight_best(s):\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: lightgreen' if v else '' for v in is_max]\n",
    "\n",
    "# Highlight best per Branch+Model group\n",
    "styled = results_df.style\n",
    "for (branch, model), group in results_df.groupby(['Branch', 'Model']):\n",
    "    idx = group['Accuracy'].idxmax()\n",
    "    styled = styled.apply(lambda x: ['background-color: lightgreen' if i == idx else '' for i in x.index], subset=['Accuracy'])\n",
    "    idx_f1 = group['F1-score'].idxmax()\n",
    "    styled = styled.apply(lambda x: ['background-color: lightblue' if i == idx_f1 else '' for i in x.index], subset=['F1-score'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STORING ALL TRAINED MODELS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# =========================\n",
    "# BRANCH A (80% train, 20% test)\n",
    "# =========================\n",
    "\n",
    "# --- No Feature Selection ---\n",
    "joblib.dump(svm_A, 'svm_A_model_not_tuned.pkl')         # SVM, Branch A, no feature selection, not tuned\n",
    "joblib.dump(svm_A_tuned, 'svm_A_model_tuned.pkl')       # SVM, Branch A, no feature selection, tuned\n",
    "joblib.dump(dt_A, 'dt_A_model_not_tuned.pkl')           # Decision Tree, Branch A, no feature selection, not tuned\n",
    "joblib.dump(dt_A_tuned, 'dt_A_model_tuned.pkl')         # Decision Tree, Branch A, no feature selection, tuned\n",
    "joblib.dump(xgb_A, 'xgb_A_model_not_tuned.pkl')         # XGBoost, Branch A, no feature selection, not tuned\n",
    "joblib.dump(xgb_A_tuned, 'xgb_A_model_tuned.pkl')       # XGBoost, Branch A, no feature selection, tuned\n",
    "joblib.dump(scaler_A, 'scaler_A.pkl')                  # StandardScaler for Branch A\n",
    "\n",
    "# --- Chi-square Feature Selection ---\n",
    "joblib.dump(svm_A_chi, 'svm_A_chi_model_not_tuned.pkl')         # SVM, Branch A, chi-square, not tuned\n",
    "joblib.dump(svm_A_chi_tuned, 'svm_A_chi_model_tuned.pkl')       # SVM, Branch A, chi-square, tuned\n",
    "joblib.dump(dt_A_chi, 'dt_A_chi_model_not_tuned.pkl')           # Decision Tree, Branch A, chi-square, not tuned\n",
    "joblib.dump(dt_A_chi_tuned, 'dt_A_chi_model_tuned.pkl')         # Decision Tree, Branch A, chi-square, tuned\n",
    "joblib.dump(xgb_A_chi, 'xgb_A_chi_model_not_tuned.pkl')         # XGBoost, Branch A, chi-square, not tuned\n",
    "joblib.dump(xgb_A_chi_tuned, 'xgb_A_chi_model_tuned.pkl')       # XGBoost, Branch A, chi-square, tuned\n",
    "joblib.dump(selector_A, 'selector_A.pkl')                       # Chi-square selector for Branch A\n",
    "joblib.dump(minmax_scaler_A, 'minmax_scaler_A.pkl')             # MinMaxScaler for chi-square, Branch A\n",
    "\n",
    "# --- Correlation Feature Selection ---\n",
    "joblib.dump(svm_A_corr, 'svm_A_corr_model_not_tuned.pkl')       # SVM, Branch A, correlation, not tuned\n",
    "joblib.dump(svm_A_corr_tuned, 'svm_A_corr_model_tuned.pkl')     # SVM, Branch A, correlation, tuned\n",
    "joblib.dump(dt_A_corr, 'dt_A_corr_model_not_tuned.pkl')         # Decision Tree, Branch A, correlation, not tuned\n",
    "joblib.dump(dt_A_corr_tuned, 'dt_A_corr_model_tuned.pkl')       # Decision Tree, Branch A, correlation, tuned\n",
    "joblib.dump(xgb_A_corr, 'xgb_A_corr_model_not_tuned.pkl')       # XGBoost, Branch A, correlation, not tuned\n",
    "joblib.dump(xgb_A_corr_tuned, 'xgb_A_corr_model_tuned.pkl')     # XGBoost, Branch A, correlation, tuned\n",
    "# Use scaler_A for correlation branch as well\n",
    "\n",
    "# =========================\n",
    "# BRANCH B (20% train, 80% test)\n",
    "# =========================\n",
    "\n",
    "# --- No Feature Selection ---\n",
    "joblib.dump(svm_B, 'svm_B_model_not_tuned.pkl')         # SVM, Branch B, no feature selection, not tuned\n",
    "joblib.dump(svm_B_tuned, 'svm_B_model_tuned.pkl')       # SVM, Branch B, no feature selection, tuned\n",
    "joblib.dump(dt_B, 'dt_B_model_not_tuned.pkl')           # Decision Tree, Branch B, no feature selection, not tuned\n",
    "joblib.dump(dt_B_tuned, 'dt_B_model_tuned.pkl')         # Decision Tree, Branch B, no feature selection, tuned\n",
    "joblib.dump(xgb_B, 'xgb_B_model_not_tuned.pkl')         # XGBoost, Branch B, no feature selection, not tuned\n",
    "joblib.dump(xgb_B_tuned, 'xgb_B_model_tuned.pkl')       # XGBoost, Branch B, no feature selection, tuned\n",
    "joblib.dump(scaler_B, 'scaler_B.pkl')                  # StandardScaler for Branch B\n",
    "\n",
    "# --- Chi-square Feature Selection ---\n",
    "joblib.dump(svm_B_chi, 'svm_B_chi_model_not_tuned.pkl')         # SVM, Branch B, chi-square, not tuned\n",
    "joblib.dump(svm_B_chi_tuned, 'svm_B_chi_model_tuned.pkl')       # SVM, Branch B, chi-square, tuned\n",
    "joblib.dump(dt_B_chi, 'dt_B_chi_model_not_tuned.pkl')           # Decision Tree, Branch B, chi-square, not tuned\n",
    "joblib.dump(dt_B_chi_tuned, 'dt_B_chi_model_tuned.pkl')         # Decision Tree, Branch B, chi-square, tuned\n",
    "joblib.dump(xgb_B_chi, 'xgb_B_chi_model_not_tuned.pkl')         # XGBoost, Branch B, chi-square, not tuned\n",
    "joblib.dump(xgb_B_chi_tuned, 'xgb_B_chi_model_tuned.pkl')       # XGBoost, Branch B, chi-square, tuned\n",
    "joblib.dump(selector_B, 'selector_B.pkl')                       # Chi-square selector for Branch B\n",
    "joblib.dump(minmax_scaler_B, 'minmax_scaler_B.pkl')             # MinMaxScaler for chi-square, Branch B\n",
    "\n",
    "# --- Correlation Feature Selection ---\n",
    "joblib.dump(svm_B_corr, 'svm_B_corr_model_not_tuned.pkl')       # SVM, Branch B, correlation, not tuned\n",
    "joblib.dump(svm_B_corr_tuned, 'svm_B_corr_model_tuned.pkl')     # SVM, Branch B, correlation, tuned\n",
    "joblib.dump(dt_B_corr, 'dt_B_corr_model_not_tuned.pkl')         # Decision Tree, Branch B, correlation, not tuned\n",
    "joblib.dump(dt_B_corr_tuned, 'dt_B_corr_model_tuned.pkl')       # Decision Tree, Branch B, correlation, tuned\n",
    "joblib.dump(xgb_B_corr, 'xgb_B_corr_model_not_tuned.pkl')       # XGBoost, Branch B, correlation, not tuned\n",
    "joblib.dump(xgb_B_corr_tuned, 'xgb_B_corr_model_tuned.pkl')     # XGBoost, Branch B, correlation, tuned\n",
    "# Use scaler_B for correlation branch as well\n",
    "\n",
    "print(\"✅ All models (tuned and not tuned), scalers, and selectors saved with clear labels!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VISUALIZATIONS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datamining-5l9yRiuj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
